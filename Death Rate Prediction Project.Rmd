---
title: "Death Rate ( Step-wise - Elastic penalized regression )"
author: "Mo'men Mohamed"
date: "April 9, 2019"
output: html_document
---


this project is about predicating the death rate per 100000 person from more than 30 predictors

to find out more about this data and description of variables plz visit this [page](https://data.world/exercises/linear-regression-exercise-1/workspace/file?filename=cancer_reg.csv)


and for any advice or recommendation plz feel free to contact me : vet.m.mohamed@gmail.com

lets start with loading our libraries 

```{r,message=F,warning=F}
library(tidyverse)
library(knitr)
library(caret)
library(car)
library(psych)
library(mice)
library(progress)
library(DMwR)
library(readr)
library(MASS)
library(pedometrics)
```
then importing the data
```{r}
data<-read.csv("./data sets/cancer.csv")

head(data)%>%kable("markdown")
```
it is important to see the structure of the data

```{r}
str(data)
```
some of data are factors and others are numeric 
but there is no need for modification 

Lets inspect the accuracy

```{r}
summary(data)
```
the summary reveal some issues with outliers and missing data 

another thing , i think that we have to exclude the county name and keep only the name of the state

```{r}
table(data$geography)%>%data.frame()%>%head()%>%kable("markdown") #here we see that the number of county is the same of the number of data 
#this wil be problamitic in our analysis 
#So i will keep the state name and remove the county name 

data$geography<-str_remove_all(string = data$geography,
                               pattern = "[:alpha:]{1,}(\\s)|[:alpha:]{1,}(\\,)|(\\s)")


head(data,10)%>%kable("markdown")
```

good, now i think that we have to test for more than 5% missing value 

```{r}
miss<-apply(data,2,function(x){
        round((sum(is.na(x))/length(x))*100,2)
})

miss
```
 here we have 3 variable with missing values 
in my opinion , the most problematic one is the one which got 75% missing
So i will Exclude it

```{r}
data<-data%>%dplyr::select(-which(miss>70))
```
OK , lets impute the rest using KNN method

```{r}
data<-cbind(knnImputation(data = data[,-c(9,13)]),binnedinc=data$binnedinc,
            geography=data$geography)%>%data.frame()
```


the next step , is my data got ouliers !!
I think using Mahalanobis will answer this question 

first excluding the categorical variable 

```{r}
num<-data%>%dplyr::select(-geography,-binnedinc)
```
getting the mahalanobis value 

```{r}
mah<-mahalanobis(x = num,center = colMeans(num),cov = cov(num))

```

Now calculating the cutoff points 

```{r}
cutoff<-qchisq(p = .99,df = ncol(num))

```


lets open the surprise box :P

```{r}
summary(mah>cutoff)
```

now we have 356 case which considered multivariate outlier 

lets save it for later use 

```{r}
outidx<-as.numeric(mah>cutoff)
```

testing additivity for multicolinearity is something crucial
so lets test for correlation more than .9

```{r}
corr<-cor(num)%>%matrix(nrow = ncol(num),ncol = ncol(num))

addit<-apply(corr,2,function(x){
        ifelse(x>=abs(.9)&x<1,paste(round(x,2),"additive",sep = " ")," ")
})

colnames(addit)<-rownames(addit)<-names(num)
corvar<-names(data)[apply(addit,2,function(x){
        str_detect(x,"additive")
})%>%apply(MARGIN = 2,any)%>%which()]

corvar

addit[corvar,corvar]%>%kable("markdown")

```

we have here number of reported cancer (avganncount) and average reported 
mortality ( avgdeathsperyear) and number of population is highly correlated 
and we see that the number of population and average of reported mortality having no sense in predicting target
death rate , so we will exclude them

```{r}
data<-data%>%dplyr::select(-popest2015,-avgdeathsperyear)
```

For the median age of males and females i will combine it together (x1+x2)/2

```{r}
data<-data%>%mutate(medianagemf=(medianagemale+medianagefemale)/2)%>%dplyr::select(-medianagefemale,-medianagemale)
```

For the median average of coverage and coverage alone and employee coverage i will make a linear function of them 
 = .3*(x1+x2+x3)

```{r}
data<-data%>%mutate(medcov=.3*(pctprivatecoverage+pctprivatecoveragealone+pctempprivcoverage))%>%dplyr::select(-pctprivatecoverage,-pctprivatecoveragealone,-pctempprivcoverage)
```

run the correlation again 

```{r}
num<-data%>%dplyr::select(-geography,-binnedinc)

corr2<-cor(num)

addit2<-apply(corr2,2,function(x){
        ifelse(x>=abs(.9)&x<1,paste(round(x,2),"additive",sep = " ")," ")
})

colnames(addit2)<-rownames(addit2)<-names(num)
corvar2<-names(data)[apply(addit2,2,function(x){
        str_detect(x,"additive")
})%>%apply(MARGIN = 2,any)%>%which()]

corvar2
```

Great, lets dive further in our analysis and run correlation with all variables

```{r}
fitall<-data%>%with(lm(target_deathrate~.,data=data))

summary(fitall)
```

Now we can run regression diagnostics

firstly we can see the leverage points 

```{r}
lev<-hatvalues(model = fitall)
```

next calculating the cutoff point = (2*K+2)/N

```{r}
k<-ncol(data)-1
hatcut<-((2*k)+2)/nrow(data)
```

lets see how many exceed the cutoff

```{r}
(lev>hatcut)%>%summary()
```

we have 1951 points which exceed the cutpoint 

lets store it in variable 

```{r}
levout<-as.numeric(lev>hatcut)
```

now we will test cook's D for leverage and dispersion 

```{r}
cook<-cooks.distance(fitall)
```

setting the cutoff point for cook's distance 

```{r}
cookcut<-(4/(nrow(data)-(ncol(data)-1)-1))
(cook>cookcut)%>%summary()
```

here we have 143 cases which considered as influence with some misbehaving values returned to NA

```{r}
cookout<-as.numeric(cook>cookcut)
```

now we need to see the cases which is considered as outlier and influence 

```{r}
totalout<-outidx+levout+cookout

table(totalout)
```

I think that removing points which got two issues is enough 

```{r}
idx<-totalout>=2

idx<-sapply(idx,function(x){
        ifelse(is.na(x),TRUE,x)
})

table(idx)

data<-data[-idx,]

```

here i think that we have to take a look on VIF for variables cause Inflation 

```{r}
viftest<-vif(fitall)

viftest<-data.frame(viftest)

summary(viftest$GVIF>10)
summary(viftest$GVIF..1..2.Df..>3.5)
```

i think that we face a great problem here regarding multicolinearity 
but i will delay any action for it to the end of our analysis

Now , lets test normality , linearity , homoscdasticity !!

```{r,warning=F,message=F}
stdresid<-rstudent(fitall)
stdfit<-fitted(fitall)%>%scale

plot(fitall,2)
plot(stdfit,stdresid)
abline(v = 0,h = 0)
```

plotting of residual carry good news regarding normality and homoscdasticity 
congrats !!

here i think that i am ready to run my model and conducting the real analysis 
but we have to not forget that we have a serious problem in co-linearity
so i will do number of models and compare between them 
these models are:
1- Model Contain all variables 
2- step-wise model regarding the multicolinear variables
3- step-wise model regarding AIC
4- Regularized model (elastic model)
and then comparing between RMSE and R^2

firstly lets convert the categorical variables to dummy variables 

```{r}
dummy<-dummyVars(target_deathrate~.,data=data)

data<-cbind(target_deathrate=data$target_deathrate,predict(dummy,data))%>%data.frame()
```

then partitioning our data 

```{r}
set.seed(234)
trainidx<-createDataPartition(data$target_deathrate,p = .7,list = F)
traindata<-data[trainidx,]
testdata<-data[-trainidx,]
```

Now lets run all models consequently and doing 10 CV fold on the training data 

```{r}
fitcontrol<-trainControl(method = "cv",number = 10)
```

all variables model 

```{r,warning=F,message=F}
allfit<-train(target_deathrate ~.,data = traindata,method="lm",trControl=fitcontrol)

summary(allfit)
```

OK , the factor predictor of states is misbehave and rarely be significant
 
 our data matrix is at least equal to the number of parameters we want to fit. One way to invoke it is having some col-linear covariates which exist in our data

Now lets test step-wise regression 

```{r,cache=T,message=F,warning=F,results=F}
stepfit<-train(target_deathrate ~.,data = traindata,method="lmStepAIC",trControl=fitcontrol)
```

Seeing the summary and the coefficients 

```{r}
summary(stepfit)
```

Now the final model is penalized model 

Penalized Model has three type : 

1- lasso model
2- ridge model
3- Elsatic Model

here i will use elastic model since we have alot of colinear variable which to need to be tru zero and others which need only to be zero

lets run elastic model 

```{r}
elasticfit<-train(target_deathrate~.,data = traindata,trControl=fitcontrol,tuneLength=10,method="glmnet")

elasticfit$bestTune # best alpha and lambda values 

coef(elasticfit$finalModel,elasticfit$bestTune$lambda)
```

finally we need to predict the test data to Extract RMSE and R2

```{r,warning=F,message=F}
allfitpred<-predict(allfit,testdata)
allfitmeasures<-data.frame(RMSE=RMSE(pred = allfitpred,obs = testdata$target_deathrate),R2=R2(pred = allfitpred,obs = testdata$target_deathrate))

stepfitpred<-predict(stepfit,testdata)
stepfitmeasures<-data.frame(RMSE=RMSE(pred = stepfitpred,obs = testdata$target_deathrate),R2=R2(pred = stepfitpred,obs = testdata$target_deathrate))

elasticfitpred<-predict(elasticfit,testdata)
elasticfitmeasures<-data.frame(RMSE=RMSE(pred = elasticfitpred,obs = testdata$target_deathrate),R2=R2(pred = elasticfitpred,obs = testdata$target_deathrate))

```

putting all measures together

```{r}
allmeasures<-rbind(allfitmeasures,stepfitmeasures,elasticfitmeasures)

rownames(allmeasures)<-c("Model with all variables","stepwise model","elastic penalized model")

allmeasures%>%kable("markdown")
```


at last 
we see here that the penalized model is the most accurate model 
in this model we reduced the coefficients of some model to be near zero and others to be zero (combination between lasso and ridge regression)
the best alpha value is .2 and lambda value is .407

this was an effort to achieve the best prediction of death rate using more than 30 variables 

Regards 